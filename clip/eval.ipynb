{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "#from tqdm.autonotebook import tqdm # if first is not working\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import pprint\n",
    "\n",
    "# for hubert-base-cc\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# for sentiment-hts2-hubert-hungarian\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# for xlm-roberta-base, distilbert-base-multilingual-cased\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4, width=200, depth=None, stream=None, compact=False, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"images_30k\"\n",
    "captions_path = \"captions\"\n",
    "file=\"captions_30k_hu\"\n",
    "csv_name = f\"{file}.csv\"\n",
    "df = pd.read_csv(f\"{captions_path}/{file}.txt\", delimiter=\"|\")\n",
    "df.columns = ['image', 'caption_number', 'caption']\n",
    "df['caption'] = df['caption']\n",
    "df['caption_number'] = df['caption_number']\n",
    "ids = [id_ for id_ in range(len(df) // 5) for i in range(5)]\n",
    "df['id'] = ids\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = image_path\n",
    "    captions_path = captions_path\n",
    "    batch_size = 32\n",
    "    num_workers = 0 # it was 4 on colab, should be 0 on our pc-s\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1 # for server running it could be 10, 30 or even 50\n",
    "    patience_val = 10\n",
    "    factor = 0.8\n",
    "    epochs = 1000 # could be 1000\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    model_name = 'tf_efficientnetv2_b1'\n",
    "    image_embedding = 1280  # for efficientnetv2b1\n",
    "\n",
    "    text_model_name = [\"SZTAKI-HLT/hubert-base-cc\", \"xlm-roberta-base\", \"NYTK/sentiment-hts2-hubert-hungarian\", \"distilbert-base-multilingual-cased\"]\n",
    "    text_encoder_model = text_model_name[1]\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = text_encoder_model\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True   # for both image encoder and text encoder\n",
    "    trainable = False   # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 240  # for efficientnetv2b1\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        #item['caption'] = self.captions[idx]   # it is not needed, just to help visualize\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained, \n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "\n",
    "        print(model_name)\n",
    "        if(CFG.text_model_name.index(model_name)==0):\n",
    "            self.model = AutoModel.from_pretrained(model_name) # hubert-base-cc\n",
    "        elif(CFG.text_model_name.index(model_name)==1 or CFG.text_model_name.index(model_name)==3):\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(model_name)  # xlm-roberta-base or distilbert-base-multilingual-cased\n",
    "        elif(CFG.text_model_name.index(model_name)==2):\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name) # sentiment-hts2-hubert-hungarian\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        if(CFG.text_model_name.index(CFG.text_encoder_model)==0):\n",
    "            output = self.model(input_ids=input_ids, attention_mask=attention_mask)    # hubert-base-cc\n",
    "            last_hidden_state = output.last_hidden_state   # hubert-base-cc\n",
    "        else:\n",
    "            output = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)    # other\n",
    "            last_hidden_state = output.hidden_states[-1]   # other\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_test_dfs():\n",
    "    dataframe = pd.read_csv(f\"{CFG.captions_path}/{csv_name}\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(image_ids)\n",
    "\n",
    "    test_ids = image_ids[int(len(image_ids)*0.8):]\n",
    "    valid_ids = image_ids[int(len(image_ids)*0.6):int(len(image_ids)*0.8)]\n",
    "    train_ids = image_ids[:int(len(image_ids)*0.6)]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    test_dataframe = dataframe[dataframe[\"id\"].isin(test_ids)].reset_index(drop=True)\n",
    "    \n",
    "    return train_dataframe, valid_dataframe, test_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=CFG.batch_size,num_workers=CFG.num_workers,shuffle=True if mode == \"train\" else False,)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_embeddings(test_df, model_path):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "\n",
    "    test_loader = build_loaders(test_df, tokenizer, mode=\"test\")\n",
    "    \n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
    "    model.eval()\n",
    "    \n",
    "    test_image_embeddings = []\n",
    "    test_text_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            # img embeddings\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            test_image_embeddings.append(image_embeddings)\n",
    "            # text embeddings\n",
    "            text_features = model.text_encoder(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            text_embeddings = model.text_projection(text_features)\n",
    "            test_text_embeddings.append(text_embeddings)\n",
    "    return model, torch.cat(test_image_embeddings), torch.cat(test_text_embeddings) # torch.cat makes a torch from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_df = make_train_valid_test_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df.to_csv(f\"test_df_from_30k.csv\", index=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_w_fnames = test_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_hubert_base_cc, image_embeddings_hubert_base_cc, text_embeddings_hubert_base_cc = get_model_embeddings(test_df, \"best_models/hubert-base-cc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_sentiment_hts2_hubert_hungarian, image_embeddings_sentiment_hts2_hubert_hungarian, text_embeddings_sentiment_hts2_hubert_hungarian = get_model_embeddings(test_df, \"best_models/sentiment-hts2-hubert-hungarian.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xlm_roberta_base, image_embeddings_xlm_roberta_base, text_embeddings_xlm_roberta_base = get_model_embeddings(test_df, \"best_models/xlm-roberta-base.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_distilbert_base_multilingual_cased, image_embeddings_distilbert_base_multilingual_cased, text_embeddings_distilbert_base_multilingual_cased = get_model_embeddings(test_df, \"best_models/distilbert-base-multilingual-cased.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_from_text_to_img(model, image_embeddings, caption, image_filenames, n=5):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "\n",
    "    encoded_query = tokenizer([caption])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(CFG.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embedding = model.text_projection(text_features)\n",
    "    \n",
    "    #print(text_embedding) embedding works fine\n",
    "    \n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embedding_n = F.normalize(text_embedding, p=2, dim=-1)\n",
    "\n",
    "    dot_similarity = text_embedding_n @ image_embeddings_n.T\n",
    "\n",
    "    #print(\"image_embeddings_n.size()\",image_embeddings_n.size())    \n",
    "    #print(\"len(image_filenames)\",len(image_filenames))  # they are the same size\n",
    "\n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "\n",
    "    match = [image_filenames[idx] for idx in indices[::5]]\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_matches_from_text_to_img(model_distilbert_base_multilingual_cased, \n",
    "#                              image_embeddings_distilbert_base_multilingual_cased, \n",
    "#                              'Egy fénykép egy lóról.', \n",
    "#                              image_filenames=test_df['image'].values, n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 1 ?\n",
    "\n",
    "print(\"len(test_df['caption'].values)\",len(test_df['caption'].values))\n",
    "cnt=0\n",
    "res=0\n",
    "test_imgs = list(test_df['image'].values)\n",
    "for e in captions_w_fnames:\n",
    "    cnt+=1\n",
    "    print(cnt, end='\\r')\n",
    "    #print(\"expected: \",e['caption'], e['image'])\n",
    "    #print(\"got: \",end='')\n",
    "    pred_img = find_matches_from_text_to_img(model_xlm_roberta_base, image_embeddings_xlm_roberta_base, e['caption'], image_filenames=test_imgs, n=1)[0]\n",
    "    #print('e['image']',e['image'])\n",
    "    #print('pred_img',pred_img)\n",
    "    \n",
    "    if pred_img == e['image']: res += 1\n",
    "    #break\n",
    "\n",
    "print(res ,'/', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_value(dict_obj, key, value):\n",
    "    # Check if key exist in dict or not\n",
    "    if key in dict_obj:\n",
    "        # Key exist in dict.\n",
    "        # Check if type of value of key is list or not\n",
    "        if not isinstance(dict_obj[key], list):\n",
    "            # If type is not list then make it list\n",
    "            dict_obj[key] = [dict_obj[key]]\n",
    "        # Append the value in list\n",
    "        dict_obj[key].append(value)\n",
    "    else:\n",
    "        # As key is not in dict,\n",
    "        # so, add key-value pair\n",
    "        dict_obj[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a dict with an image name, and 5 captions\n",
    "d_imgs_w_texts = {} \n",
    "for e in captions_w_fnames:\n",
    "    append_value(d_imgs_w_texts, e['image'], e['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(d_imgs_w_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_from_img_to_texts(model, text_embeddings, image_fn, captions, n=5):\n",
    "\n",
    "    transforms = get_transforms(mode='test')\n",
    "    image = cv2.imread(f\"{CFG.image_path}/{image_fn}\")\n",
    "    image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #img = mpimg.imread(f\"{CFG.image_path}/{image_fn}\")\n",
    "    #imgplot = plt.imshow(img)\n",
    "    #plt.show() # too much img printing can destroy memory\n",
    "\n",
    "    #print(\"len(captions)\",len(captions))\n",
    "    #print(\"text_embeddings.shape\",text_embeddings.shape)   # same len\n",
    "    image = transforms(image=image)['image']\n",
    "    image = torch.tensor(image).permute(2, 0, 1).float()\n",
    "    image_features = model.image_encoder(image.unsqueeze(0).to(CFG.device))\n",
    "    image_embedding = model.image_projection(image_features)\n",
    "    \n",
    "    #print(image_embedding) # embedding is the same\n",
    "\n",
    "    image_embedding_n = F.normalize(image_embedding, p=2, dim=-1)   # just one image\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)   # all caption \n",
    "    dot_similarity = image_embedding_n @ text_embeddings_n.T\n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n)\n",
    "    #print(\"indices\",indices)  # (best) indexes\n",
    "    #print(\"values\",values)   # (best) similarity values\n",
    "    matches = [captions[idx] for idx in indices]\n",
    "    \n",
    "    #print(image_fn,matches)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "res_sum = 0\n",
    "cnt_when_not_zero=0\n",
    "all_cnt = len(d_imgs_w_texts)*5\n",
    "for k,v  in d_imgs_w_texts.items():\n",
    "    #print(cnt,end='\\r')\n",
    "    res=0\n",
    "    # k = img_fname v = list_of_captions_for_the_img\n",
    "    zs=find_matches_from_img_to_texts(model_xlm_roberta_base, text_embeddings_xlm_roberta_base, k, captions=test_df['caption'].values, n=5)\n",
    "    #print(\"expected: \",k, v)\n",
    "    #print(\"got: \", zs)\n",
    "    for text in zs:\n",
    "        if(text in v): res+=1\n",
    "    #print(f'{res}/5')\n",
    "    if res != 0 : cnt_when_not_zero+=1 \n",
    "    res_sum += res\n",
    "    cnt+=1\n",
    "    #break\n",
    "\n",
    "print(cnt_when_not_zero,'/',len(d_imgs_w_texts))\n",
    "print(res_sum ,'/', all_cnt)\n",
    "\n",
    "# 5 shot?\n",
    "\n",
    "# model_distilbert_base_multilingual_cased\n",
    "# 321 / 6357\n",
    "# 366 / 31785\n",
    "\n",
    "# model_sentiment_hts2_hubert_hungarian\n",
    "# 677 / 6357\n",
    "# 815 / 31785 \n",
    "\n",
    "# model hubert_base_cc\n",
    "# 706 / 6357\n",
    "# 845 / 31785\n",
    "\n",
    "# model_xlm_roberta_base\n",
    "# 452 / 6357\n",
    "# 520 / 31785"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6787011c45a3efbe649b58d2421ae9152c81bafd697e8826c2692fe659a45695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
