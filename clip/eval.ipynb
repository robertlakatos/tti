{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\L0021\\Anaconda3\\envs\\tti\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "#from tqdm.autonotebook import tqdm # if first is not working\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "# for hubert-base-cc\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# for sentiment-hts2-hubert-hungarian\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# for xlm-roberta-base\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption_number</th>\n",
       "      <th>caption</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Két bozontos hajú fiatal srác nézi a kezüket, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Két fiatal, fehér hím van kint sok bokor mellett.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Két zöld inges férfi áll az udvaron.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Egy férfi kék ingben áll a kertben.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Két barát élvezi az együtt töltött időt.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158910</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Egy rövidnadrágos és hawaii inges férfi egy pi...</td>\n",
       "      <td>31782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158911</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Fiatal férfi lóg egy csónak oldala fölött, ami...</td>\n",
       "      <td>31782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158912</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Egy férfi egy kék - fehér csónak oldaláról dől...</td>\n",
       "      <td>31782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158913</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Egy férfi egy kis csónakon lovagol egy kikötőb...</td>\n",
       "      <td>31782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158914</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Egy férfi egy kikötött kék - fehér csónakon , ...</td>\n",
       "      <td>31782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158915 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image  caption_number  \\\n",
       "0       1000092795.jpg               0   \n",
       "1       1000092795.jpg               1   \n",
       "2       1000092795.jpg               2   \n",
       "3       1000092795.jpg               3   \n",
       "4       1000092795.jpg               4   \n",
       "...                ...             ...   \n",
       "158910   998845445.jpg               0   \n",
       "158911   998845445.jpg               1   \n",
       "158912   998845445.jpg               2   \n",
       "158913   998845445.jpg               3   \n",
       "158914   998845445.jpg               4   \n",
       "\n",
       "                                                  caption     id  \n",
       "0       Két bozontos hajú fiatal srác nézi a kezüket, ...      0  \n",
       "1       Két fiatal, fehér hím van kint sok bokor mellett.      0  \n",
       "2                    Két zöld inges férfi áll az udvaron.      0  \n",
       "3                     Egy férfi kék ingben áll a kertben.      0  \n",
       "4                Két barát élvezi az együtt töltött időt.      0  \n",
       "...                                                   ...    ...  \n",
       "158910  Egy rövidnadrágos és hawaii inges férfi egy pi...  31782  \n",
       "158911  Fiatal férfi lóg egy csónak oldala fölött, ami...  31782  \n",
       "158912  Egy férfi egy kék - fehér csónak oldaláról dől...  31782  \n",
       "158913  Egy férfi egy kis csónakon lovagol egy kikötőb...  31782  \n",
       "158914  Egy férfi egy kikötött kék - fehér csónakon , ...  31782  \n",
       "\n",
       "[158915 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"images_30k\"\n",
    "captions_path = \"captions\"\n",
    "file=\"captions_30k_hu\"\n",
    "csv_name = f\"{file}.csv\"\n",
    "df = pd.read_csv(f\"{captions_path}/{file}.txt\", delimiter=\"|\")\n",
    "df.columns = ['image', 'caption_number', 'caption']\n",
    "df['caption'] = df['caption']\n",
    "df['caption_number'] = df['caption_number']\n",
    "ids = [id_ for id_ in range(len(df) // 5) for i in range(5)]\n",
    "df['id'] = ids\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = image_path\n",
    "    captions_path = captions_path\n",
    "    batch_size = 32\n",
    "    num_workers = 0 # it was 4 on colab, should be 0 on our pc-s\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1 # for server running it could be 10, 30 or even 50\n",
    "    patience_val = 10\n",
    "    factor = 0.8\n",
    "    epochs = 1000 # could be 1000\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    model_name = 'tf_efficientnetv2_b1'\n",
    "    image_embedding = 1280  # for efficientnetv2b1\n",
    "\n",
    "    text_model_name = [\"SZTAKI-HLT/hubert-base-cc\", \"xlm-roberta-base\", \"NYTK/sentiment-hts2-hubert-hungarian\"]\n",
    "    text_encoder_model = text_model_name[2]\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = text_encoder_model\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True   # for both image encoder and text encoder\n",
    "    trainable = False   # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 240  # for efficientnetv2b1\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        #item['caption'] = self.captions[idx]   # it is not needed, just to help visualize\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained, \n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "\n",
    "        print(model_name)\n",
    "        if(CFG.text_model_name.index(model_name)==0):\n",
    "            self.model = AutoModel.from_pretrained(model_name) # hubert-base-cc\n",
    "        elif(CFG.text_model_name.index(model_name)==1):\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(model_name)  # xlm-roberta-base\n",
    "        elif(CFG.text_model_name.index(model_name)==2):\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name) # sentiment-hts2-hubert-hungarian\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        if(CFG.text_model_name.index(CFG.text_encoder_model)==0):\n",
    "            output = self.model(input_ids=input_ids, attention_mask=attention_mask)    # hubert-base-cc\n",
    "            last_hidden_state = output.last_hidden_state   # hubert-base-cc\n",
    "        else:\n",
    "            output = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)    # other\n",
    "            last_hidden_state = output.hidden_states[-1]   # other\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{CFG.captions_path}/{csv_name}\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(image_ids)\n",
    "    #valid_ids = np.random.choice(image_ids, size=int(0.2 * len(image_ids)), replace=False)\n",
    "    #train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    test_ids = image_ids[int(len(image_ids)*0.8):]\n",
    "    valid_ids = image_ids[int(len(image_ids)*0.6):int(len(image_ids)*0.8)]\n",
    "    train_ids = image_ids[:int(len(image_ids)*0.6)]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    test_dataframe = dataframe[dataframe[\"id\"].isin(test_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe, test_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=CFG.batch_size,num_workers=CFG.num_workers,shuffle=True if mode == \"train\" else False,)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(valid_df, model_path):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, valid_df, test_df = make_train_valid_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hubert_base_cc, image_embeddings_hubert_base_cc = get_image_embeddings(test_df, \"best_models/hubert-base-cc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment_hts2_hubert_hungarian, image_embeddings_sentiment_hts2_hubert_hungarian = get_image_embeddings(test_df, \"best_models/sentiment-hts2-hubert-hungarian.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xlm_roberta_base, image_embeddings_xlm_roberta_base = get_image_embeddings(test_df, \"best_models/xlm-roberta-base.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tti')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beaffad3c664d4de30fc08e6a8e7baa9cb2bc593d2908a0f9c4eb9612e35c0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
