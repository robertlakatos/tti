{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all text encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4, width=200, depth=None, stream=None, compact=False, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('szinonimak.json', encoding='utf-8') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make smaller test dict\n",
    "d_keys_li=list(d.keys())\n",
    "smaller_dict={}\n",
    "for i in range(0,10):\n",
    "    smaller_dict[d_keys_li[i]]=d[d_keys_li[i]]\n",
    "# pp.pprint(smaller_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d - full test disctionary\n",
    "# smaller_dict - 10 key dictionary\n",
    "DICT = smaller_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_res(tokenizer,model):\n",
    "    from scipy.spatial import distance\n",
    "\n",
    "    sum=0\n",
    "    cnt=0\n",
    "    avg=0\n",
    "\n",
    "    for k,v in DICT.items():\n",
    "        baseword=k\n",
    "        encoded_input_base=tokenizer(baseword,return_tensors='pt')\n",
    "        output_base = model(**encoded_input_base)\n",
    "        for e in v[1]:\n",
    "            synonym=e\n",
    "            encoded_input_synonym=tokenizer(synonym,return_tensors='pt')\n",
    "            output_synonym = model(**encoded_input_synonym)\n",
    "\n",
    "            embedded_base=output_base[0][0][-1]\n",
    "            embedded_synonym=output_synonym[0][0][-1]\n",
    "            dist=distance.cosine(embedded_base.detach().numpy(),embedded_synonym.detach().numpy())\n",
    "            sum=sum+dist\n",
    "            cnt=cnt+1\n",
    "    # overall result on synonym-dictionary       \n",
    "    avg=sum/cnt\n",
    "    print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_res_sentiment(tokenizer, model):\n",
    "    from scipy.spatial import distance\n",
    "\n",
    "    sum=0\n",
    "    cnt=0\n",
    "    avg=0\n",
    "\n",
    "    for k,v in DICT.items():\n",
    "        baseword=k\n",
    "        encoded_input_base=tokenizer(baseword,return_tensors='pt')\n",
    "        output_base = model(**encoded_input_base)\n",
    "        for e in v[1]:\n",
    "            synonym=e\n",
    "            encoded_input_synonym=tokenizer(synonym,return_tensors='pt')\n",
    "            output_synonym = model(**encoded_input_synonym)\n",
    "\n",
    "            embedded_base=output_base[0][-1]\n",
    "            embedded_synonym=output_synonym[0][-1]\n",
    "            dist=distance.cosine(embedded_base.tolist(),embedded_synonym.tolist())\n",
    "            sum=sum+dist\n",
    "            cnt=cnt+1\n",
    "    # overall result on synonym-dictionary       \n",
    "    avg=sum/cnt\n",
    "    print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_res_xlm100(tokenizer,model):\n",
    "    from scipy.spatial import distance\n",
    "\n",
    "    sum=0\n",
    "    cnt=0\n",
    "    avg=0\n",
    "\n",
    "    language_id_hu = tokenizer.lang2id[\"hu\"]\n",
    "\n",
    "    for k,v in DICT.items():\n",
    "        baseword=k\n",
    "        base_input_ids = torch.tensor([tokenizer.encode(baseword)])\n",
    "        base_lang =  torch.tensor([language_id_hu] * base_input_ids.shape[1])\n",
    "        base_lang = base_lang.view(1, -1)\n",
    "        output_base = model(base_input_ids, langs=base_lang)\n",
    "        for e in v[1]:\n",
    "            synonym=e\n",
    "            syn_input_ids = torch.tensor([tokenizer.encode(synonym)])\n",
    "            syn_lang =  torch.tensor([language_id_hu] * syn_input_ids.shape[1])\n",
    "            syn_lang = syn_lang.view(1, -1)\n",
    "            output_syn = model(syn_input_ids, langs=syn_lang)\n",
    "\n",
    "            embedded_base=output_base[0][0][-1]\n",
    "            embedded_syn=output_syn[0][0][-1]\n",
    "            dist=distance.cosine(embedded_base.tolist(),embedded_syn.tolist())\n",
    "            sum=sum+dist\n",
    "            cnt=cnt+1\n",
    "    # overall result on synonym-dictionary       \n",
    "    avg=sum/cnt\n",
    "    print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miki\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading vocab.txt: 100%|██████████| 851k/851k [00:00<00:00, 1.10MB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading config.json: 100%|██████████| 625/625 [00:00<00:00, 156kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 641M/641M [00:12<00:00, 53.1MB/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2021213493176869\n"
     ]
    }
   ],
   "source": [
    "# bert-base-multilingual-uncased\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading vocab.txt: 100%|██████████| 972k/972k [00:03<00:00, 278kB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 7.25kB/s]\n",
      "Downloading config.json: 100%|██████████| 625/625 [00:00<00:00, 209kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 681M/681M [02:00<00:00, 5.95MB/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3758295446026082\n"
     ]
    }
   ],
   "source": [
    "# bert-base-multilingual-cased\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 596/596 [00:00<00:00, 149kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 591k/591k [00:01<00:00, 377kB/s] \n",
      "Downloading merges.txt: 100%|██████████| 376k/376k [00:01<00:00, 331kB/s]  \n",
      "Downloading tokenizer.json: 100%|██████████| 1.01M/1.01M [00:03<00:00, 321kB/s]\n",
      "Downloading special_tokens_map.json: 100%|██████████| 109/109 [00:00<00:00, 36.4kB/s]\n",
      "Downloading config.json: 100%|██████████| 773/773 [00:00<00:00, 257kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 436M/436M [00:08<00:00, 53.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04946080944976028\n"
     ]
    }
   ],
   "source": [
    "# text-generation-news-gpt2-small-hungarian\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/text-generation-news-gpt2-small-hungarian\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NYTK/text-generation-news-gpt2-small-hungarian\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 596/596 [00:00<00:00, 149kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 591k/591k [00:01<00:00, 355kB/s] \n",
      "Downloading merges.txt: 100%|██████████| 376k/376k [00:01<00:00, 311kB/s] \n",
      "Downloading tokenizer.json: 100%|██████████| 1.01M/1.01M [00:02<00:00, 405kB/s] \n",
      "Downloading special_tokens_map.json: 100%|██████████| 109/109 [00:00<00:00, 21.8kB/s]\n",
      "Downloading config.json: 100%|██████████| 773/773 [00:00<00:00, 194kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 436M/436M [00:08<00:00, 54.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04117110006663264\n"
     ]
    }
   ],
   "source": [
    "# text-generation-poem-petofi-gpt2-small-hungarian\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/text-generation-poem-petofi-gpt2-small-hungarian\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NYTK/text-generation-poem-petofi-gpt2-small-hungarian\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 420/420 [00:00<00:00, 105kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 424M/424M [00:08<00:00, 52.8MB/s] \n",
      "Some weights of the model checkpoint at SZTAKI-HLT/hubert-base-cc were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading tokenizer_config.json: 100%|██████████| 86.0/86.0 [00:00<00:00, 43.1kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 266k/266k [00:00<00:00, 340kB/s]  \n",
      "Downloading special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 28.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0043750046467294496\n"
     ]
    }
   ],
   "source": [
    "# hubert-base-cc\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 725/725 [00:00<00:00, 182kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 1.81M/1.81M [00:02<00:00, 654kB/s]\n",
      "Downloading merges.txt: 100%|██████████| 1.15M/1.15M [00:01<00:00, 711kB/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.21G/3.21G [00:55<00:00, 61.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007187895020660089\n"
     ]
    }
   ],
   "source": [
    "# mGPT\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/mGPT\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/mGPT\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 1.07k/1.07k [00:00<00:00, 549kB/s]\n",
      "Downloading config.json: 100%|██████████| 761/761 [00:00<00:00, 380kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 500k/500k [00:02<00:00, 233kB/s]  \n",
      "Downloading merges.txt: 100%|██████████| 305k/305k [00:00<00:00, 540kB/s]  \n",
      "Downloading tokenizer.json: 100%|██████████| 864k/864k [00:01<00:00, 621kB/s]  \n",
      "Downloading special_tokens_map.json: 100%|██████████| 772/772 [00:00<00:00, 774kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 254M/254M [00:05<00:00, 48.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7105878830825121\n"
     ]
    }
   ],
   "source": [
    "# sentiment-hts2-xlm-roberta-hungarian\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/sentiment-hts2-xlm-roberta-hungarian\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"NYTK/sentiment-hts2-xlm-roberta-hungarian\")\n",
    "\n",
    "get_avg_res_sentiment(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 394/394 [00:00<00:00, 197kB/s]\n",
      "Downloading config.json: 100%|██████████| 1.00k/1.00k [00:00<00:00, 511kB/s]\n",
      "Downloading sentencepiece.bpe.model: 100%|██████████| 4.83M/4.83M [00:00<00:00, 9.55MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 8.66M/8.66M [00:04<00:00, 1.86MB/s]\n",
      "Downloading special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 47.8kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.04G/1.04G [00:38<00:00, 29.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14574735969674552\n"
     ]
    }
   ],
   "source": [
    "# sentiment-hts5-xlm-roberta-hungarian\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/sentiment-hts5-xlm-roberta-hungarian\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"NYTK/sentiment-hts5-xlm-roberta-hungarian\")\n",
    "\n",
    "get_avg_res_sentiment(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 346/346 [00:00<00:00, 69.3kB/s]\n",
      "Downloading config.json: 100%|██████████| 911/911 [00:00<00:00, 228kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 266k/266k [00:00<00:00, 598kB/s] \n",
      "Downloading tokenizer.json: 100%|██████████| 507k/507k [00:00<00:00, 902kB/s]  \n",
      "Downloading special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 28.0kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 422M/422M [00:09<00:00, 47.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22346660085836195\n"
     ]
    }
   ],
   "source": [
    "# sentiment-hts5-hubert-hungarian\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/sentiment-hts5-hubert-hungarian\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"NYTK/sentiment-hts5-hubert-hungarian\")\n",
    "\n",
    "get_avg_res_sentiment(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 346/346 [00:00<00:00, 49.4kB/s]\n",
      "Downloading config.json: 100%|██████████| 681/681 [00:00<00:00, 136kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 266k/266k [00:00<00:00, 571kB/s] \n",
      "Downloading tokenizer.json: 100%|██████████| 507k/507k [00:00<00:00, 927kB/s]  \n",
      "Downloading special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 28.1kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 422M/422M [00:08<00:00, 51.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41677919550617826\n"
     ]
    }
   ],
   "source": [
    "# sentiment-hts2-hubert-hungarian\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/sentiment-hts2-hubert-hungarian\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"NYTK/sentiment-hts2-hubert-hungarian\")\n",
    "\n",
    "get_avg_res_sentiment(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading vocab.json: 100%|██████████| 878k/878k [00:00<00:00, 1.29MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 446k/446k [00:00<00:00, 833kB/s] \n",
      "Downloading config.json: 100%|██████████| 481/481 [00:00<00:00, 80.2kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 478M/478M [00:18<00:00, 26.7MB/s] \n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015747680956003617\n"
     ]
    }
   ],
   "source": [
    "# roberta-base\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0042058442320142475\n"
     ]
    }
   ],
   "source": [
    "# xlm-roberta-base\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 616/616 [00:00<00:00, 205kB/s]\n",
      "Downloading sentencepiece.bpe.model: 100%|██████████| 4.83M/4.83M [00:06<00:00, 725kB/s] \n",
      "Downloading tokenizer.json: 100%|██████████| 8.68M/8.68M [00:03<00:00, 2.53MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 2.09G/2.09G [00:38<00:00, 58.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006280950137547084\n"
     ]
    }
   ],
   "source": [
    "# xlm-roberta-large\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "get_avg_res(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading vocab.json: 100%|██████████| 5.45M/5.45M [00:03<00:00, 1.83MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 2.84M/2.84M [00:01<00:00, 2.33MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 2.29k/2.29k [00:00<00:00, 470kB/s]\n",
      "Downloading config.json: 100%|██████████| 41.0k/41.0k [00:00<00:00, 181kB/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.06G/1.06G [00:27<00:00, 42.2MB/s]\n",
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-100-1280 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3686501758554123\n"
     ]
    }
   ],
   "source": [
    "# xlm-mlm-100-1280\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "\n",
    "get_avg_res_xlm100(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miki\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08749399410218608\n"
     ]
    }
   ],
   "source": [
    "# xlm-roberta-xl\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# xxl - 40G+\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/xlm-roberta-xl\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/xlm-roberta-xl\")\n",
    "\n",
    "get_avg_res(tokenizer,model)\n",
    "# 10 key dict - 8 min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
